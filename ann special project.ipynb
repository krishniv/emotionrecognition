{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import np_utils\n",
    "import os,glob,pickle\n",
    "import librosa\n",
    "\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Dropout, GaussianNoise, Conv1D\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracting do not use\n",
    "def extract(data,sr):\n",
    "    \n",
    "    stft=np.abs(librosa.stft(data))\n",
    "    result=np.array([])\n",
    "    mfccs=np.mean(librosa.feature.mfcc(y=data, sr=sr, n_mfcc=40).T, axis=0)\n",
    "    result=np.hstack((result, mfccs))\n",
    "    chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sr).T,axis=0)\n",
    "    result=np.hstack((result, chroma))\n",
    "    mel=np.mean(librosa.feature.melspectrogram(data, sr=sr).T,axis=0)\n",
    "    result=np.hstack((result, mel))\n",
    "    \n",
    "    \n",
    "    return result\n",
    "emotions={\n",
    "  1:'neutral',\n",
    "  2:'calm',\n",
    "  3:'happy',\n",
    "  4:'sad',\n",
    "  5:'angry',\n",
    "  6:'fearful',\n",
    "  7:'disgust',\n",
    "  8:'surprised'}\n",
    "x=[]\n",
    "y=[]\n",
    "\n",
    "#path name to be specified\n",
    "for file in glob.glob(r\"C:\\Users\\DELL\\Desktop\\projct\\mldata\\Actor_*\\\\*.wav\"):#give path for the dataset\n",
    "    f=os.path.basename(file)\n",
    "    emotion=f.split(\"-\")[2]\n",
    "    data,sr=sf.read(file,dtype=\"float32\")\n",
    "    features=extract(data,sr)\n",
    "    x.append(features)\n",
    "    y.append(emotion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cumulative explained variance')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#train_test_split,encoding,scaler\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(x,y,random_state=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "scaler.fit(X_test)\n",
    "X_sc_train = scaler.transform(X_train)\n",
    "X_sc_test = scaler.transform(X_test)\n",
    "le=LabelEncoder()\n",
    "y_test=le.fit_transform(y_test)\n",
    "y_train=le.fit_transform(y_train)\n",
    "\n",
    "pca = PCA(n_components=180)\n",
    "pca.fit(X_sc_train)\n",
    "\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.519011</td>\n",
       "      <td>-0.029527</td>\n",
       "      <td>0.560156</td>\n",
       "      <td>-1.962477</td>\n",
       "      <td>-1.690145</td>\n",
       "      <td>0.338671</td>\n",
       "      <td>2.883784</td>\n",
       "      <td>-0.153616</td>\n",
       "      <td>-0.409595</td>\n",
       "      <td>-1.249788</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030418</td>\n",
       "      <td>-0.063060</td>\n",
       "      <td>-0.163882</td>\n",
       "      <td>-0.067705</td>\n",
       "      <td>-0.181329</td>\n",
       "      <td>-0.025264</td>\n",
       "      <td>-0.042473</td>\n",
       "      <td>0.211848</td>\n",
       "      <td>-0.108651</td>\n",
       "      <td>-0.414437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.813085</td>\n",
       "      <td>-0.727379</td>\n",
       "      <td>-0.893807</td>\n",
       "      <td>1.869317</td>\n",
       "      <td>1.686283</td>\n",
       "      <td>-1.894197</td>\n",
       "      <td>-3.213508</td>\n",
       "      <td>1.087941</td>\n",
       "      <td>-0.978512</td>\n",
       "      <td>-1.199431</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175873</td>\n",
       "      <td>0.189716</td>\n",
       "      <td>-0.543452</td>\n",
       "      <td>-0.190916</td>\n",
       "      <td>-0.036198</td>\n",
       "      <td>0.086784</td>\n",
       "      <td>-0.192380</td>\n",
       "      <td>-0.397190</td>\n",
       "      <td>-0.361841</td>\n",
       "      <td>0.047331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.397959</td>\n",
       "      <td>3.846877</td>\n",
       "      <td>-1.909592</td>\n",
       "      <td>4.140775</td>\n",
       "      <td>6.619879</td>\n",
       "      <td>-1.666509</td>\n",
       "      <td>3.804926</td>\n",
       "      <td>-14.192289</td>\n",
       "      <td>-7.139220</td>\n",
       "      <td>-4.516089</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.466945</td>\n",
       "      <td>0.046991</td>\n",
       "      <td>-0.114867</td>\n",
       "      <td>-0.357074</td>\n",
       "      <td>-0.480712</td>\n",
       "      <td>0.388053</td>\n",
       "      <td>0.285033</td>\n",
       "      <td>0.694021</td>\n",
       "      <td>-0.576909</td>\n",
       "      <td>0.180223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.786599</td>\n",
       "      <td>-0.019709</td>\n",
       "      <td>0.355529</td>\n",
       "      <td>-1.487574</td>\n",
       "      <td>-2.133292</td>\n",
       "      <td>-0.408266</td>\n",
       "      <td>2.893350</td>\n",
       "      <td>1.619350</td>\n",
       "      <td>-0.517659</td>\n",
       "      <td>0.034968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060077</td>\n",
       "      <td>-0.280693</td>\n",
       "      <td>0.232598</td>\n",
       "      <td>-0.181155</td>\n",
       "      <td>-0.221962</td>\n",
       "      <td>-0.410359</td>\n",
       "      <td>-0.058753</td>\n",
       "      <td>-0.106725</td>\n",
       "      <td>-0.085423</td>\n",
       "      <td>0.113001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.489223</td>\n",
       "      <td>-0.148614</td>\n",
       "      <td>0.257400</td>\n",
       "      <td>-0.535260</td>\n",
       "      <td>-0.933514</td>\n",
       "      <td>-0.497012</td>\n",
       "      <td>0.988519</td>\n",
       "      <td>2.154222</td>\n",
       "      <td>-0.716543</td>\n",
       "      <td>1.079247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204574</td>\n",
       "      <td>0.094153</td>\n",
       "      <td>-0.173915</td>\n",
       "      <td>-0.062894</td>\n",
       "      <td>0.136676</td>\n",
       "      <td>0.515138</td>\n",
       "      <td>0.541629</td>\n",
       "      <td>-0.133023</td>\n",
       "      <td>0.008825</td>\n",
       "      <td>-0.332299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>-3.505158</td>\n",
       "      <td>-0.012941</td>\n",
       "      <td>0.631674</td>\n",
       "      <td>-2.407199</td>\n",
       "      <td>-2.581458</td>\n",
       "      <td>0.144886</td>\n",
       "      <td>3.433603</td>\n",
       "      <td>0.267390</td>\n",
       "      <td>-0.291457</td>\n",
       "      <td>-0.520071</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194514</td>\n",
       "      <td>-0.134767</td>\n",
       "      <td>-0.164536</td>\n",
       "      <td>-0.217829</td>\n",
       "      <td>0.090803</td>\n",
       "      <td>0.047691</td>\n",
       "      <td>-0.124489</td>\n",
       "      <td>0.276530</td>\n",
       "      <td>-0.138392</td>\n",
       "      <td>0.134149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>-2.878445</td>\n",
       "      <td>-0.198072</td>\n",
       "      <td>-0.540969</td>\n",
       "      <td>1.826062</td>\n",
       "      <td>1.242072</td>\n",
       "      <td>-0.649675</td>\n",
       "      <td>-2.744067</td>\n",
       "      <td>0.834804</td>\n",
       "      <td>-0.159321</td>\n",
       "      <td>1.045351</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038544</td>\n",
       "      <td>0.050282</td>\n",
       "      <td>0.118103</td>\n",
       "      <td>0.271434</td>\n",
       "      <td>-0.163448</td>\n",
       "      <td>-0.121305</td>\n",
       "      <td>-0.213313</td>\n",
       "      <td>-0.056632</td>\n",
       "      <td>0.009506</td>\n",
       "      <td>0.108660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>-0.869213</td>\n",
       "      <td>0.773143</td>\n",
       "      <td>1.617120</td>\n",
       "      <td>-1.854730</td>\n",
       "      <td>0.355767</td>\n",
       "      <td>1.056049</td>\n",
       "      <td>2.146414</td>\n",
       "      <td>1.592738</td>\n",
       "      <td>-0.352845</td>\n",
       "      <td>0.330211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072151</td>\n",
       "      <td>0.255422</td>\n",
       "      <td>0.123695</td>\n",
       "      <td>-0.095030</td>\n",
       "      <td>-0.170961</td>\n",
       "      <td>0.432397</td>\n",
       "      <td>-0.014879</td>\n",
       "      <td>-0.332022</td>\n",
       "      <td>0.272889</td>\n",
       "      <td>0.185184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>33.267912</td>\n",
       "      <td>9.803032</td>\n",
       "      <td>22.917530</td>\n",
       "      <td>15.191324</td>\n",
       "      <td>16.789295</td>\n",
       "      <td>8.605229</td>\n",
       "      <td>11.973011</td>\n",
       "      <td>6.842334</td>\n",
       "      <td>3.531190</td>\n",
       "      <td>-25.401766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522961</td>\n",
       "      <td>0.588668</td>\n",
       "      <td>-0.222359</td>\n",
       "      <td>-0.089683</td>\n",
       "      <td>0.723156</td>\n",
       "      <td>0.150911</td>\n",
       "      <td>-0.045300</td>\n",
       "      <td>0.342786</td>\n",
       "      <td>0.092729</td>\n",
       "      <td>0.454942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>-1.623442</td>\n",
       "      <td>0.191012</td>\n",
       "      <td>-0.039850</td>\n",
       "      <td>1.643147</td>\n",
       "      <td>0.683791</td>\n",
       "      <td>0.067447</td>\n",
       "      <td>-1.888770</td>\n",
       "      <td>-1.312626</td>\n",
       "      <td>0.411796</td>\n",
       "      <td>-2.513546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071725</td>\n",
       "      <td>0.116092</td>\n",
       "      <td>-0.212895</td>\n",
       "      <td>0.150856</td>\n",
       "      <td>0.439416</td>\n",
       "      <td>-0.259338</td>\n",
       "      <td>-0.444429</td>\n",
       "      <td>-0.718326</td>\n",
       "      <td>-0.334826</td>\n",
       "      <td>0.577905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1080 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1          2          3          4         5   \\\n",
       "0     -2.519011 -0.029527   0.560156  -1.962477  -1.690145  0.338671   \n",
       "1      1.813085 -0.727379  -0.893807   1.869317   1.686283 -1.894197   \n",
       "2     16.397959  3.846877  -1.909592   4.140775   6.619879 -1.666509   \n",
       "3     -3.786599 -0.019709   0.355529  -1.487574  -2.133292 -0.408266   \n",
       "4     -3.489223 -0.148614   0.257400  -0.535260  -0.933514 -0.497012   \n",
       "...         ...       ...        ...        ...        ...       ...   \n",
       "1075  -3.505158 -0.012941   0.631674  -2.407199  -2.581458  0.144886   \n",
       "1076  -2.878445 -0.198072  -0.540969   1.826062   1.242072 -0.649675   \n",
       "1077  -0.869213  0.773143   1.617120  -1.854730   0.355767  1.056049   \n",
       "1078  33.267912  9.803032  22.917530  15.191324  16.789295  8.605229   \n",
       "1079  -1.623442  0.191012  -0.039850   1.643147   0.683791  0.067447   \n",
       "\n",
       "             6          7         8          9   ...        90        91  \\\n",
       "0      2.883784  -0.153616 -0.409595  -1.249788  ...  0.030418 -0.063060   \n",
       "1     -3.213508   1.087941 -0.978512  -1.199431  ... -0.175873  0.189716   \n",
       "2      3.804926 -14.192289 -7.139220  -4.516089  ... -0.466945  0.046991   \n",
       "3      2.893350   1.619350 -0.517659   0.034968  ...  0.060077 -0.280693   \n",
       "4      0.988519   2.154222 -0.716543   1.079247  ...  0.204574  0.094153   \n",
       "...         ...        ...       ...        ...  ...       ...       ...   \n",
       "1075   3.433603   0.267390 -0.291457  -0.520071  ... -0.194514 -0.134767   \n",
       "1076  -2.744067   0.834804 -0.159321   1.045351  ... -0.038544  0.050282   \n",
       "1077   2.146414   1.592738 -0.352845   0.330211  ...  0.072151  0.255422   \n",
       "1078  11.973011   6.842334  3.531190 -25.401766  ...  0.522961  0.588668   \n",
       "1079  -1.888770  -1.312626  0.411796  -2.513546  ...  0.071725  0.116092   \n",
       "\n",
       "            92        93        94        95        96        97        98  \\\n",
       "0    -0.163882 -0.067705 -0.181329 -0.025264 -0.042473  0.211848 -0.108651   \n",
       "1    -0.543452 -0.190916 -0.036198  0.086784 -0.192380 -0.397190 -0.361841   \n",
       "2    -0.114867 -0.357074 -0.480712  0.388053  0.285033  0.694021 -0.576909   \n",
       "3     0.232598 -0.181155 -0.221962 -0.410359 -0.058753 -0.106725 -0.085423   \n",
       "4    -0.173915 -0.062894  0.136676  0.515138  0.541629 -0.133023  0.008825   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1075 -0.164536 -0.217829  0.090803  0.047691 -0.124489  0.276530 -0.138392   \n",
       "1076  0.118103  0.271434 -0.163448 -0.121305 -0.213313 -0.056632  0.009506   \n",
       "1077  0.123695 -0.095030 -0.170961  0.432397 -0.014879 -0.332022  0.272889   \n",
       "1078 -0.222359 -0.089683  0.723156  0.150911 -0.045300  0.342786  0.092729   \n",
       "1079 -0.212895  0.150856  0.439416 -0.259338 -0.444429 -0.718326 -0.334826   \n",
       "\n",
       "            99  \n",
       "0    -0.414437  \n",
       "1     0.047331  \n",
       "2     0.180223  \n",
       "3     0.113001  \n",
       "4    -0.332299  \n",
       "...        ...  \n",
       "1075  0.134149  \n",
       "1076  0.108660  \n",
       "1077  0.185184  \n",
       "1078  0.454942  \n",
       "1079  0.577905  \n",
       "\n",
       "[1080 rows x 100 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA\n",
    "pca=PCA(n_components=100,svd_solver ='randomized')\n",
    "X_pca_train=pca.fit_transform(X_sc_train)\n",
    "X_pca_test=pca.transform(X_sc_test)\n",
    "pca_std = np.std(X_pca_train)\n",
    "pd.DataFrame(X_pca_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 5.1860 - accuracy: 0.1259\n",
      "Epoch 2/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 4.4490 - accuracy: 0.1741\n",
      "Epoch 3/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 4.0836 - accuracy: 0.1731\n",
      "Epoch 4/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 3.9042 - accuracy: 0.1833\n",
      "Epoch 5/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 3.6690 - accuracy: 0.1741\n",
      "Epoch 6/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 3.4443 - accuracy: 0.1815\n",
      "Epoch 7/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 3.1806 - accuracy: 0.1880\n",
      "Epoch 8/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 3.0435 - accuracy: 0.1944\n",
      "Epoch 9/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2.9003 - accuracy: 0.2028\n",
      "Epoch 10/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 2.6763 - accuracy: 0.2222\n",
      "Epoch 11/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 2.4949 - accuracy: 0.2472\n",
      "Epoch 12/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 2.3946 - accuracy: 0.2722\n",
      "Epoch 13/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 2.2702 - accuracy: 0.2815\n",
      "Epoch 14/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 2.2143 - accuracy: 0.2676\n",
      "Epoch 15/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2.0655 - accuracy: 0.2981\n",
      "Epoch 16/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 2.0368 - accuracy: 0.2722\n",
      "Epoch 17/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 1.9119 - accuracy: 0.2944\n",
      "Epoch 18/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 1.8505 - accuracy: 0.3324\n",
      "Epoch 19/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 1.8454 - accuracy: 0.3148\n",
      "Epoch 20/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 1.8374 - accuracy: 0.3204\n",
      "Epoch 21/200\n",
      "34/34 [==============================] - ETA: 0s - loss: 1.7348 - accuracy: 0.36 - 0s 4ms/step - loss: 1.7225 - accuracy: 0.3546\n",
      "Epoch 22/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 1.6908 - accuracy: 0.3426\n",
      "Epoch 23/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.6821 - accuracy: 0.3463\n",
      "Epoch 24/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 1.6580 - accuracy: 0.3546\n",
      "Epoch 25/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 1.6103 - accuracy: 0.3824\n",
      "Epoch 26/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 1.6090 - accuracy: 0.3796\n",
      "Epoch 27/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 1.5352 - accuracy: 0.4167\n",
      "Epoch 28/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 1.5726 - accuracy: 0.3889\n",
      "Epoch 29/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 1.5256 - accuracy: 0.4056\n",
      "Epoch 30/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.4857 - accuracy: 0.4204\n",
      "Epoch 31/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.5013 - accuracy: 0.4176\n",
      "Epoch 32/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 1.4905 - accuracy: 0.4194\n",
      "Epoch 33/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.4661 - accuracy: 0.4370\n",
      "Epoch 34/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 1.4370 - accuracy: 0.4509\n",
      "Epoch 35/200\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 1.4060 - accuracy: 0.4694\n",
      "Epoch 36/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.4027 - accuracy: 0.4611\n",
      "Epoch 37/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 1.3984 - accuracy: 0.4657\n",
      "Epoch 38/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.4227 - accuracy: 0.4444\n",
      "Epoch 39/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.3830 - accuracy: 0.4889\n",
      "Epoch 40/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.3590 - accuracy: 0.4769\n",
      "Epoch 41/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 1.3769 - accuracy: 0.4861\n",
      "Epoch 42/200\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 1.3539 - accuracy: 0.4898\n",
      "Epoch 43/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.3302 - accuracy: 0.4991\n",
      "Epoch 44/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.3630 - accuracy: 0.4861\n",
      "Epoch 45/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.3172 - accuracy: 0.4935\n",
      "Epoch 46/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.3395 - accuracy: 0.5056\n",
      "Epoch 47/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.3087 - accuracy: 0.5185\n",
      "Epoch 48/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.3074 - accuracy: 0.5046\n",
      "Epoch 49/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.3427 - accuracy: 0.4898\n",
      "Epoch 50/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.2953 - accuracy: 0.5046\n",
      "Epoch 51/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.2630 - accuracy: 0.5370\n",
      "Epoch 52/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.2196 - accuracy: 0.5398\n",
      "Epoch 53/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.2521 - accuracy: 0.5343\n",
      "Epoch 54/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.2344 - accuracy: 0.5546\n",
      "Epoch 55/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.2498 - accuracy: 0.5370\n",
      "Epoch 56/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.2250 - accuracy: 0.5491\n",
      "Epoch 57/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.2187 - accuracy: 0.5546\n",
      "Epoch 58/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.2059 - accuracy: 0.5361\n",
      "Epoch 59/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.2050 - accuracy: 0.5500\n",
      "Epoch 60/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.1943 - accuracy: 0.5620\n",
      "Epoch 61/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.1748 - accuracy: 0.5759\n",
      "Epoch 62/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 1.1334 - accuracy: 0.5991\n",
      "Epoch 63/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.1729 - accuracy: 0.5778\n",
      "Epoch 64/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.1460 - accuracy: 0.5639\n",
      "Epoch 65/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.1556 - accuracy: 0.5694\n",
      "Epoch 66/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 1.1438 - accuracy: 0.5769\n",
      "Epoch 67/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.0741 - accuracy: 0.6130\n",
      "Epoch 68/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.1369 - accuracy: 0.5741\n",
      "Epoch 69/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1.0916 - accuracy: 0.5843\n",
      "Epoch 70/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.1306 - accuracy: 0.5685\n",
      "Epoch 71/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.1463 - accuracy: 0.5824\n",
      "Epoch 72/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.0955 - accuracy: 0.5963\n",
      "Epoch 73/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.0988 - accuracy: 0.5944\n",
      "Epoch 74/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.0547 - accuracy: 0.6139\n",
      "Epoch 75/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.0812 - accuracy: 0.6204\n",
      "Epoch 76/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.0695 - accuracy: 0.5981\n",
      "Epoch 77/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.0429 - accuracy: 0.6315\n",
      "Epoch 78/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.0134 - accuracy: 0.6370\n",
      "Epoch 79/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.0222 - accuracy: 0.6148\n",
      "Epoch 80/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.0175 - accuracy: 0.6269\n",
      "Epoch 81/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.0635 - accuracy: 0.6028\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 5ms/step - loss: 1.0022 - accuracy: 0.6426\n",
      "Epoch 83/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1.0267 - accuracy: 0.6250\n",
      "Epoch 84/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.9893 - accuracy: 0.6370\n",
      "Epoch 85/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.9905 - accuracy: 0.6472\n",
      "Epoch 86/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.9491 - accuracy: 0.6519\n",
      "Epoch 87/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.9577 - accuracy: 0.6454\n",
      "Epoch 88/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 1.0045 - accuracy: 0.6454\n",
      "Epoch 89/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.9954 - accuracy: 0.6333\n",
      "Epoch 90/200\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.9213 - accuracy: 0.6611\n",
      "Epoch 91/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.9786 - accuracy: 0.6546\n",
      "Epoch 92/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.8888 - accuracy: 0.6667\n",
      "Epoch 93/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.9401 - accuracy: 0.6667\n",
      "Epoch 94/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.8780 - accuracy: 0.6861\n",
      "Epoch 95/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.8965 - accuracy: 0.6769\n",
      "Epoch 96/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.8995 - accuracy: 0.6731\n",
      "Epoch 97/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.9454 - accuracy: 0.6602\n",
      "Epoch 98/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.9266 - accuracy: 0.6787\n",
      "Epoch 99/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.9273 - accuracy: 0.6537\n",
      "Epoch 100/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.8848 - accuracy: 0.6870\n",
      "Epoch 101/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.8710 - accuracy: 0.6750\n",
      "Epoch 102/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.8709 - accuracy: 0.6870\n",
      "Epoch 103/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.8727 - accuracy: 0.6880\n",
      "Epoch 104/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.8528 - accuracy: 0.6963\n",
      "Epoch 105/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.8703 - accuracy: 0.6778\n",
      "Epoch 106/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.8050 - accuracy: 0.7222\n",
      "Epoch 107/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.8595 - accuracy: 0.7019\n",
      "Epoch 108/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.8184 - accuracy: 0.7000\n",
      "Epoch 109/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.8000 - accuracy: 0.7120\n",
      "Epoch 110/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.7922 - accuracy: 0.7250\n",
      "Epoch 111/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.8095 - accuracy: 0.7148\n",
      "Epoch 112/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.7729 - accuracy: 0.7111\n",
      "Epoch 113/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.8363 - accuracy: 0.7083\n",
      "Epoch 114/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.7889 - accuracy: 0.7120: 0s - loss: 0.6805 - accuracy: \n",
      "Epoch 115/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.7611 - accuracy: 0.7352\n",
      "Epoch 116/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.7262 - accuracy: 0.7352: 0s - loss: 0.7381 - accuracy: 0.72\n",
      "Epoch 117/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.8160 - accuracy: 0.7046\n",
      "Epoch 118/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.7888 - accuracy: 0.7176\n",
      "Epoch 119/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.7284 - accuracy: 0.7269\n",
      "Epoch 120/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.7238 - accuracy: 0.7269\n",
      "Epoch 121/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.7909 - accuracy: 0.7185\n",
      "Epoch 122/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.7481 - accuracy: 0.7398\n",
      "Epoch 123/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6623 - accuracy: 0.7491\n",
      "Epoch 124/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.7292 - accuracy: 0.7463\n",
      "Epoch 125/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6904 - accuracy: 0.7565\n",
      "Epoch 126/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6956 - accuracy: 0.7500\n",
      "Epoch 127/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6923 - accuracy: 0.7685\n",
      "Epoch 128/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.7209 - accuracy: 0.7417\n",
      "Epoch 129/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6810 - accuracy: 0.7537\n",
      "Epoch 130/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.7335 - accuracy: 0.7481\n",
      "Epoch 131/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6759 - accuracy: 0.7583\n",
      "Epoch 132/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6791 - accuracy: 0.7667\n",
      "Epoch 133/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6661 - accuracy: 0.7509\n",
      "Epoch 134/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6771 - accuracy: 0.7528\n",
      "Epoch 135/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6900 - accuracy: 0.7509\n",
      "Epoch 136/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6774 - accuracy: 0.7574\n",
      "Epoch 137/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6165 - accuracy: 0.7787\n",
      "Epoch 138/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5957 - accuracy: 0.7778\n",
      "Epoch 139/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6998 - accuracy: 0.7574\n",
      "Epoch 140/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5960 - accuracy: 0.7981\n",
      "Epoch 141/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6017 - accuracy: 0.7963\n",
      "Epoch 142/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6219 - accuracy: 0.7963\n",
      "Epoch 143/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5842 - accuracy: 0.8037\n",
      "Epoch 144/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6096 - accuracy: 0.7861\n",
      "Epoch 145/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5565 - accuracy: 0.8046\n",
      "Epoch 146/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5850 - accuracy: 0.7898\n",
      "Epoch 147/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.6233 - accuracy: 0.7870\n",
      "Epoch 148/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.6125 - accuracy: 0.7907\n",
      "Epoch 149/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.6041 - accuracy: 0.7981\n",
      "Epoch 150/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5532 - accuracy: 0.7991\n",
      "Epoch 151/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5925 - accuracy: 0.8093\n",
      "Epoch 152/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5749 - accuracy: 0.8037\n",
      "Epoch 153/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5358 - accuracy: 0.8065\n",
      "Epoch 154/200\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.5125 - accuracy: 0.8130\n",
      "Epoch 155/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5604 - accuracy: 0.8157\n",
      "Epoch 156/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.5233 - accuracy: 0.8194\n",
      "Epoch 157/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5297 - accuracy: 0.8167\n",
      "Epoch 158/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5034 - accuracy: 0.8306\n",
      "Epoch 159/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.4894 - accuracy: 0.8389\n",
      "Epoch 160/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.5111 - accuracy: 0.8176\n",
      "Epoch 161/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.5384 - accuracy: 0.8102\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 5ms/step - loss: 0.4923 - accuracy: 0.8269\n",
      "Epoch 163/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.4993 - accuracy: 0.8241\n",
      "Epoch 164/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.4886 - accuracy: 0.8491\n",
      "Epoch 165/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.4983 - accuracy: 0.8426\n",
      "Epoch 166/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.4987 - accuracy: 0.8287\n",
      "Epoch 167/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.4699 - accuracy: 0.8389\n",
      "Epoch 168/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.5068 - accuracy: 0.8287\n",
      "Epoch 169/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.4797 - accuracy: 0.8380\n",
      "Epoch 170/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.4927 - accuracy: 0.8361\n",
      "Epoch 171/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.4405 - accuracy: 0.8444\n",
      "Epoch 172/200\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.4830 - accuracy: 0.8380\n",
      "Epoch 173/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.4260 - accuracy: 0.8519\n",
      "Epoch 174/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.4302 - accuracy: 0.8593\n",
      "Epoch 175/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.4033 - accuracy: 0.8500\n",
      "Epoch 176/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.4136 - accuracy: 0.8648\n",
      "Epoch 177/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.4222 - accuracy: 0.8593\n",
      "Epoch 178/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.4561 - accuracy: 0.8398\n",
      "Epoch 179/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.4705 - accuracy: 0.8426\n",
      "Epoch 180/200\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.4516 - accuracy: 0.8491\n",
      "Epoch 181/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.4072 - accuracy: 0.8648\n",
      "Epoch 182/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.4000 - accuracy: 0.8759\n",
      "Epoch 183/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.3872 - accuracy: 0.8667\n",
      "Epoch 184/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.3527 - accuracy: 0.8796\n",
      "Epoch 185/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.4623 - accuracy: 0.8491\n",
      "Epoch 186/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.4409 - accuracy: 0.8583\n",
      "Epoch 187/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.4557 - accuracy: 0.8500\n",
      "Epoch 188/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.3784 - accuracy: 0.8787\n",
      "Epoch 189/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.4503 - accuracy: 0.8630\n",
      "Epoch 190/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.3834 - accuracy: 0.8704\n",
      "Epoch 191/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.4131 - accuracy: 0.8574\n",
      "Epoch 192/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.3806 - accuracy: 0.8806\n",
      "Epoch 193/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.4110 - accuracy: 0.8731\n",
      "Epoch 194/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.3416 - accuracy: 0.8861\n",
      "Epoch 195/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.3361 - accuracy: 0.8824\n",
      "Epoch 196/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.3113 - accuracy: 0.8926\n",
      "Epoch 197/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.3596 - accuracy: 0.8824\n",
      "Epoch 198/200\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 0.3373 - accuracy: 0.8861\n",
      "Epoch 199/200\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.3427 - accuracy: 0.8944\n",
      "Epoch 200/200\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.4013 - accuracy: 0.8787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e8fd62c370>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model creation\n",
    "model =tf.keras.Sequential()\n",
    "layers = 4\n",
    "units = 256\n",
    "\n",
    "model.add(Dense(units, input_dim=100, activation='relu'))\n",
    "model.add(GaussianNoise(pca_std))\n",
    "for i in range(layers):\n",
    "    model.add(Dense(units, activation='relu'))\n",
    "    model.add(GaussianNoise(pca_std))\n",
    "    model.add(Dropout(0.1))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_pca_train, y_train, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0569450855255127, 0.6527777910232544]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_pca_test,y_test,verbose=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting\n",
    "y_pred=model.predict(X_pca_test) \n",
    "y_pred=y_pred.argmax(axis=1)\n",
    "#y_pred=le.inverse_transform(y_pred)\n",
    "#y_test=le.inverse_transform(y_test)\n",
    "#printing\n",
    "for l in y_pred:\n",
    "    #print(emotions[l+1])\n",
    "    continue\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using gradient boosting\n",
    "gbc=GradientBoostingClassifier(learning_rate=0.2,max_depth=5,random_state=0).fit(X_pca_train,y_train)\n",
    "\n",
    "y_pred=gbc.predict(X_pca_test)\n",
    "accuracy=accuracy_score(y_test,y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model and weights at C:\\Users\\DELL\\ML\\saved_models\\Emotion_Model.h5 \n"
     ]
    }
   ],
   "source": [
    "#Saving model\n",
    "model_name = 'Emotion_Model.h5'\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Save model and weights at %s ' % model_path)\n",
    "\n",
    "# Save the model to disk\n",
    "model_json = model.to_json()\n",
    "with open(\"model_json.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
